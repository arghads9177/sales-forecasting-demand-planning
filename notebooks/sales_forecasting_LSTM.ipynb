{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db5b2d1-b33c-43c3-a0e2-16c6b0c7320b",
   "metadata": {},
   "source": [
    "# Sales Analysis, Forecasting and Demand Planning Project  \n",
    "\n",
    "## Project Overview  \n",
    "This project focuses on developing a robust sales forecasting and demand planning system for retail businesses. By analyzing historical sales data, we aim to optimize production, inventory management, and resource planning by accurately predicting future sales trends. Leveraging machine learning and statistical models, this project provides actionable insights to improve operational efficiency, reduce costs, and enhance customer satisfaction.\n",
    "\n",
    "---\n",
    "\n",
    "## About the Dataset  \n",
    "\n",
    "This dataset provides historical sales data for the retail furniture sector, serving as a valuable resource for business analysis. It includes detailed transaction-level information that can be used to understand sales trends, forecast future demand, and optimize inventory. The dataset enables informed decision-making to ensure business stability and growth in the competitive retail environment.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "This sales data is available on Kaggle in the following Link,\n",
    "\n",
    "> https://www.kaggle.com/datasets/tanayatipre/store-sales-forecasting-dataset\n",
    "\n",
    "### Dataset Features  \n",
    "\n",
    "| Feature          | Description                                                                          |\n",
    "|-------------------|--------------------------------------------------------------------------------------|\n",
    "| `Row ID`         | Sequential identifier for each row.                                                 |\n",
    "| `Order ID`       | Unique identifier for each sales order.                                             |\n",
    "| `Order Date`     | Date of the sales order.                                                            |\n",
    "| `Ship Date`      | Date of shipment for the order.                                                     |\n",
    "| `Ship Mode`      | Mode of shipment for the order.                                                     |\n",
    "| `Customer ID`    | Unique identifier for each customer.                                                |\n",
    "| `Customer Name`  | Name of the customer.                                                               |\n",
    "| `Segment`        | Segment classification of the customer.                                             |\n",
    "| `Country`        | Country where the sale occurred.                                                    |\n",
    "| `City`           | City where the sale occurred.                                                       |\n",
    "| `State`          | State where the sale occurred.                                                      |\n",
    "| `Postal Code`    | Postal code where the sale occurred.                                                |\n",
    "| `Region`         | Geographical region where the sale occurred.                                        |\n",
    "| `Product ID`     | Unique identifier for each product.                                                 |\n",
    "| `Category`       | Category classification of the product.                                             |\n",
    "| `Sub-Category`   | Sub-category classification of the product.                                         |\n",
    "| `Product Name`   | Name of the product.                                                                |\n",
    "| `Sales`          | Total sales amount for the order.                                                   |\n",
    "| `Quantity`       | Quantity of products sold in the order.                                             |\n",
    "| `Discount`       | Discount applied to the order.                                                      |\n",
    "| `Profit`         | Profit generated from the order.                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "## Business Objectives  \n",
    "\n",
    "1. **Sales Forecasting:**  \n",
    "   - Predict sales for the next 30 days for each product category.  \n",
    "   - Identify and leverage trends and seasonality in sales patterns.  \n",
    "\n",
    "2. **Demand Planning:**  \n",
    "   - Determine products or categories likely to experience surges in demand.  \n",
    "   - Reduce overstocking and understocking through accurate forecasts.  \n",
    "\n",
    "3. **Optimization:**  \n",
    "   - Optimize production schedules and inventory management.  \n",
    "   - Identify periods requiring special promotions to counter seasonal declines.  \n",
    "\n",
    "---\n",
    "\n",
    "## Methodology  \n",
    "\n",
    "### 1. **Data Understanding**  \n",
    "   - **Data Collection:** Gather historical sales data, pricing, promotions, holidays, and external factors (e.g., weather).  \n",
    "   - **Exploratory Data Analysis (EDA):** Perform statistical analysis and create visualizations to uncover trends, seasonality, and anomalies.  \n",
    "   - **Data Quality Assessment:** Identify and address missing, inconsistent, or irrelevant data.  \n",
    "\n",
    "### 2. **Data Preparation**  \n",
    "   - **Data Cleaning:** Handle missing values, outliers, and duplicates. Normalize sales data if necessary.  \n",
    "   - **Feature Engineering:** Create lag variables, rolling averages, seasonal indices, and encode categorical variables for modeling.  \n",
    "   - **Data Splitting:** Split the dataset into training, validation, and testing sets.  \n",
    "\n",
    "### 3. **Modeling**  \n",
    "   - **Baseline Models:** Develop simple models such as moving averages or exponential smoothing for benchmarking.  \n",
    "   - **Advanced Models:** Train machine learning (e.g., ARIMA, SARIMA, XGBoost, Random Forest) and deep learning models (e.g., LSTM, GRU, Prophet).  \n",
    "   - **Hyperparameter Optimization:** Fine-tune models to enhance accuracy and efficiency.  \n",
    "\n",
    "### 4. **Evaluation**  \n",
    "   - **Evaluation Metrics:** Use RMSE, MAPE, MAE, and R² to assess model performance.  \n",
    "   - **Visualization:** Plot predicted vs. actual sales to analyze trends and deviations.  \n",
    "   - **Model Selection:** Choose the best-performing model for deployment.  \n",
    "\n",
    "---\n",
    "\n",
    "## Applications  \n",
    "\n",
    "- **Inventory Management:** Ensure optimal inventory levels, minimizing costs associated with overstocking or stockouts.  \n",
    "- **Production Planning:** Use forecasts to adjust production schedules based on predicted demand.  \n",
    "- **Promotional Campaigns:** Identify low-demand periods and design targeted promotions to boost sales.  \n",
    "- **Revenue Forecasting:** Provide accurate revenue projections to guide financial planning.  \n",
    "\n",
    "---\n",
    "\n",
    "## Research Questions  \n",
    "\n",
    "1. What are the expected sales for the next 30 days for each product category?  \n",
    "2. Which products or categories show clear trends or seasonal demand patterns?  \n",
    "3. How can accurate demand forecasts improve inventory management and reduce operational costs?  \n",
    "4. Which time periods require targeted promotional strategies to mitigate sales dips?  \n",
    "\n",
    "---\n",
    "\n",
    "## Results and Insights  \n",
    "\n",
    "1. **Seasonal Trends:** Sales demonstrate clear peaks during holiday seasons and dips during specific months.  \n",
    "2. **Top-Selling Products:** Analysis of product categories reveals best-performing items and their contribution to revenue.  \n",
    "3. **Demand Surges:** Certain products experience predictable spikes in demand, enabling proactive inventory management.  \n",
    "4. **Model Performance:** LSTM and SARIMA models outperformed baseline methods in forecasting accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion  \n",
    "\n",
    "This project equips retail businesses with powerful forecasting tools to make data-driven decisions. By understanding historical sales patterns, businesses can optimize inventory, enhance production efficiency, and maximize profitability while maintaining customer satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6007cea-9fd7-4e5c-8c46-e9c7f98cd98b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06486638-9450-4f87-9d17-5d57814ffde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 08:09:13.508247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe4ab2-3e61-4885-8b5f-20ca2a0dc3b1",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3957429b-1f09-4d72-becb-1425d0efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plot\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Path\n",
    "data_path = \"../data\"\n",
    "model_path = \"../models\"\n",
    "# csv_path = os.path.join(data_path, \"ssf_monthly.csv\")\n",
    "csv_path = os.path.join(data_path, \"ssf_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491107e-2cdf-47ca-9ac8-c7a170deb787",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92fdfae-1f0f-4a7f-9055-39e29a3fdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff740c6-96eb-438c-93c0-00d08fb9defb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>2573.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>76.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-08</td>\n",
       "      <td>68.465333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>60.202667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>51.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Sales\n",
       "0  2014-01-06  2573.820000\n",
       "1  2014-01-07    76.728000\n",
       "2  2014-01-08    68.465333\n",
       "3  2014-01-09    60.202667\n",
       "4  2014-01-10    51.940000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406898b4-3674-4a34-8c71-23bae67e0440",
   "metadata": {},
   "source": [
    "### Define Functions for Data Preprocessing, Model Preparation, Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a2bb2e8-e8fc-4d28-9680-0044a4bd758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "def prepare_data(data, feature):\n",
    "    data = data[feature].values.reshape(-1, 1)\n",
    "    sc = MinMaxScaler(feature_range= (0,1))\n",
    "    data_scaled = sc.fit_transform(data)\n",
    "    return data_scaled, sc\n",
    "\n",
    "# Create sequences\n",
    "def create_sequence(data, time_steps= 10):\n",
    "    X, y = [], []\n",
    "    for i in range(time_steps, len(data)):\n",
    "        X.append(data[i - time_steps:i, 0])  # Sequence of time_steps\n",
    "        y.append(data[i, 0])   # Next value after the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split the data in train and test set\n",
    "def train_test_split(X, y, train_split_at= 0.8):\n",
    "    train_size = int(len(X) * train_split_at)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Build and compile model\n",
    "def build_compile_lstm(X):\n",
    "    # Initialize model\n",
    "    model = Sequential([\n",
    "        Input(shape= (X.shape[1], 1)), # Input Layer\n",
    "        LSTM(50, return_sequences= True), # Hidden LSTM Layer 1\n",
    "        Dropout(0.2), # Dropout layer 1\n",
    "        LSTM(50, return_sequences= True), # Hidden LSTM Layer 1\n",
    "        Dropout(0.2), # Dropout layer 1\n",
    "        LSTM(50, return_sequences= True), # Hidden LSTM Layer 2\n",
    "        Dropout(0.2), # Dropout Layer 2\n",
    "        Dense(1) # Output Layer\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss= \"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "# Train Model\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs= 50, batch_size= 16):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs= epochs, batch_size= batch_size, validation_data= (X_test, y_test))\n",
    "    return model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, sc, X_train, y_train, X_test, y_test):\n",
    "    # Make Predictions on train\n",
    "    y_pred = model.predict(X_train)\n",
    "    # print(\"Scaler input features:\", sc.n_features_in_)\n",
    "    # print(\"y_pred shape:\", y_pred.shape)\n",
    "    # print(\"y_test shape:\", y_test.shape)\n",
    "    y_pred_last = y_pred[:, -1, :] # Extract the last shape\n",
    "    # print(\"y_pred_2d shape:\", y_pred_last.shape)\n",
    "    y_pred = sc.inverse_transform(y_pred_last)\n",
    "    y_train = y_train.reshape(-1, 1)  # Shape: (num_samples, 1)\n",
    "    y_train = sc.inverse_transform(y_train)\n",
    "\n",
    "    # Filter out the small values to avoid division by small values\n",
    "    threshold= 1\n",
    "    valid_indices = y_train > threshold\n",
    "    y_train_filtered= y_train[valid_indices]\n",
    "    y_pred_last_filtered = y_pred_last[valid_indices]\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_train_filtered, y_pred_last_filtered))\n",
    "    mape = mean_absolute_percentage_error(y_train_filtered, y_pred_last_filtered)\n",
    "    mae = mean_absolute_error(y_train_filtered, y_pred_last_filtered)\n",
    "    # score = r2_score(y_train_filtered, y_pred_last_filtered)\n",
    "    # Print metrics\n",
    "    print(f\"RMSE: {rmse: 0.4f}\")\n",
    "    print(f\"MAPE: {mape * 100: 0.2f}\")\n",
    "    print(f\"MAE: {mae: 0.4f}\")\n",
    "    # print(f\"Score: {score: 0.2f}\")\n",
    "    # Make Predictions on test\n",
    "    y_pred = model.predict(X_test)\n",
    "    # print(\"Scaler input features:\", sc.n_features_in_)\n",
    "    # print(\"y_pred shape:\", y_pred.shape)\n",
    "    # print(\"y_test shape:\", y_test.shape)\n",
    "    y_pred_last = y_pred[:, -1, :] # Extract the last shape\n",
    "    # print(\"y_pred_2d shape:\", y_pred_last.shape)\n",
    "    y_pred = sc.inverse_transform(y_pred_last)\n",
    "    y_test = y_test.reshape(-1, 1)  # Shape: (num_samples, 1)\n",
    "    y_test = sc.inverse_transform(y_test)\n",
    "\n",
    "    # Filter out the small values to avoid division by small values\n",
    "    threshold= 1\n",
    "    valid_indices = y_test > threshold\n",
    "    y_test_filtered= y_test[valid_indices]\n",
    "    y_pred_last_filtered = y_pred_last[valid_indices]\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_filtered, y_pred_last_filtered))\n",
    "    mape = mean_absolute_percentage_error(y_test_filtered, y_pred_last_filtered)\n",
    "    mae = mean_absolute_error(y_test_filtered, y_pred_last_filtered)\n",
    "    # score = r2_score(y_test_filtered, y_pred_last_filtered)\n",
    "    # Print metrics\n",
    "    print(f\"RMSE: {rmse: 0.4f}\")\n",
    "    print(f\"MAPE: {mape * 100: 0.2f}\")\n",
    "    print(f\"MAE: {mae: 0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0ce13-8705-4acb-905e-848d5169a615",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66ddd532-ec51-43a2-9f9b-2d49d106447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scaler = prepare_data(df, \"Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc79a56c-42ab-4153-9f35-abe404563a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1450, 5)\n",
      "Shape of y: (1450,)\n"
     ]
    }
   ],
   "source": [
    "# Create Data sequence\n",
    "X, y = create_sequence(data, 5)\n",
    "# Sanity check\n",
    "print(f\"Shape of X: {X.shape}\") # Expected: (num_samples, time_steps, num_features)\n",
    "print(f\"Shape of y: {y.shape}\") # Expexted(num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc54ad27-82d4-4130-a5b7-df512eefd12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1160, 5)\n",
      "Shape of y_train: (1160,)\n",
      "Shape of X_test: (290, 5)\n",
      "Shape of y_test: (290,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Sanity check\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9fad6-31bf-4f49-a662-4ccfc4a4f521",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09c6ea1e-6e67-447c-9f55-1dad6bfffd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,851</span> (198.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,851\u001b[0m (198.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,851</span> (198.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,851\u001b[0m (198.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_compile_lstm(X_train)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95fc885d-85cb-4b7f-9db0-b4bca62f484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 2/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0095\n",
      "Epoch 3/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 4/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 5/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 6/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 7/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 8/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 9/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0073 - val_loss: 0.0093\n",
      "Epoch 10/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 11/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 12/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0106 - val_loss: 0.0091\n",
      "Epoch 13/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 14/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0072 - val_loss: 0.0092\n",
      "Epoch 15/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 16/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 17/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 18/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 19/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0093\n",
      "Epoch 20/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 21/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 22/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 23/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0073 - val_loss: 0.0092\n",
      "Epoch 24/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 25/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 26/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 27/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 28/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0071 - val_loss: 0.0092\n",
      "Epoch 29/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 30/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 31/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 32/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0079 - val_loss: 0.0091\n",
      "Epoch 33/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 34/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 35/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 36/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 37/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 38/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 39/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 40/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 41/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0079 - val_loss: 0.0091\n",
      "Epoch 42/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 43/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0085 - val_loss: 0.0091\n",
      "Epoch 44/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 45/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 46/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 47/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0106 - val_loss: 0.0091\n",
      "Epoch 48/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0079 - val_loss: 0.0091\n",
      "Epoch 49/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 50/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 51/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0092\n",
      "Epoch 52/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0092\n",
      "Epoch 53/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 54/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0077 - val_loss: 0.0092\n",
      "Epoch 55/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069 - val_loss: 0.0092\n",
      "Epoch 56/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0072 - val_loss: 0.0092\n",
      "Epoch 57/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0101 - val_loss: 0.0093\n",
      "Epoch 58/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 59/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 60/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0089 - val_loss: 0.0092\n",
      "Epoch 61/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 62/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 63/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 64/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 65/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 66/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 67/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 68/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 69/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 70/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 71/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 72/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 73/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 74/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 75/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0078 - val_loss: 0.0093\n",
      "Epoch 76/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 77/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 78/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 79/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 80/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 81/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 82/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 83/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0092\n",
      "Epoch 84/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 85/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0092\n",
      "Epoch 86/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 87/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 88/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 89/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 90/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 91/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 92/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0078 - val_loss: 0.0092\n",
      "Epoch 93/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0084 - val_loss: 0.0091\n",
      "Epoch 94/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 95/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0082 - val_loss: 0.0092\n",
      "Epoch 96/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 97/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 98/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0092\n",
      "Epoch 99/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 100/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 101/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0085 - val_loss: 0.0091\n",
      "Epoch 102/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 103/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 104/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 105/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 106/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 107/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 108/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 109/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 110/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 111/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0097 - val_loss: 0.0092\n",
      "Epoch 112/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 113/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 114/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 115/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 116/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 117/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0079 - val_loss: 0.0091\n",
      "Epoch 118/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 119/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0101 - val_loss: 0.0092\n",
      "Epoch 120/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 121/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 122/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 123/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0077 - val_loss: 0.0092\n",
      "Epoch 124/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 125/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 126/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0077 - val_loss: 0.0092\n",
      "Epoch 127/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0074 - val_loss: 0.0092\n",
      "Epoch 128/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 129/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0099 - val_loss: 0.0091\n",
      "Epoch 130/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0081 - val_loss: 0.0092\n",
      "Epoch 131/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 132/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 133/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 134/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 135/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 136/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 137/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0092\n",
      "Epoch 138/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 139/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 140/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 141/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 142/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 143/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 144/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 145/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 146/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 147/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 148/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 149/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0092\n",
      "Epoch 150/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 151/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 152/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 153/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 154/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 155/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 156/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 157/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 158/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 159/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 160/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 161/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 162/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 163/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 164/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 165/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 166/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 167/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0089 - val_loss: 0.0091\n",
      "Epoch 168/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 169/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 170/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0087 - val_loss: 0.0091\n",
      "Epoch 171/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0084 - val_loss: 0.0091\n",
      "Epoch 172/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 173/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 174/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 175/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 176/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 177/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 178/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 179/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 180/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0106 - val_loss: 0.0092\n",
      "Epoch 181/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 182/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 183/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0088 - val_loss: 0.0091\n",
      "Epoch 184/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 185/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0085 - val_loss: 0.0092\n",
      "Epoch 186/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 187/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 188/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 189/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 190/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 191/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 192/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 193/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 194/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 195/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0097 - val_loss: 0.0092\n",
      "Epoch 196/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 197/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 198/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 199/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 200/200\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0088 - val_loss: 0.0092\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, epochs = 200, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7da10e5b-1148-42db-9cd3-680d9645833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "RMSE:  1159.3138\n",
      "MAPE:  99.94\n",
      "MAE:  757.2444\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "RMSE:  1280.0017\n",
      "MAPE:  99.92\n",
      "MAE:  857.0533\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, scaler, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef15db-0a3a-4af9-b1d6-5c9169e29924",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "#### High Error Rates (MAPE):\n",
    "\n",
    "- Both training and testing MAPE are near 100%, which is extremely high. This indicates that the model struggles to accurately capture the proportional changes in the time series.\n",
    "- The model fails to generalize well to unseen data, suggesting issues with either:\n",
    "    - Model complexity (insufficient learning capacity for this dataset).\n",
    "    - Data preprocessing or the inherent nature of the data.\n",
    "\n",
    "#### Overfitting or Underfitting:\n",
    "\n",
    "- The difference between training and testing metrics is small, indicating that the model may be underfitting the data rather than overfitting.\n",
    "- This might occur because the model complexity (50 units per layer) is not sufficient to learn intricate patterns in the sales data.\n",
    "\n",
    "#### MAE and RMSE Insights:\n",
    "\n",
    "- The RMSE values **(1159.31 for training, 1280.00 for testing)** show that the error magnitude is substantial in both cases.\n",
    "- The slightly higher testing RMSE suggests a marginal degradation in performance on unseen data, a typical sign of limited model capacity.\n",
    "\n",
    "#### Effect of Hyperparameters:\n",
    "\n",
    "- Despite varying epochs and batch sizes, the high MAPE persists. This suggests that neither increasing training duration nor tuning batch sizes effectively improves the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90728270-6fc7-4b7d-9270-6e0b948e42da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
